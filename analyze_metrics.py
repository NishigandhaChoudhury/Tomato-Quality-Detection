"""
Model Performance Metrics Analysis
Red or Ripe: ML-Based Tomato Quality Grading
"""

import numpy as np
import json

print("="*70)
print("üéØ RED OR RIPE - MODEL PERFORMANCE METRICS")
print("="*70)

# Load training results
try:
    with open('results/training_history.json', 'r') as f:
        history = json.load(f)
except:
    print("‚ö†Ô∏è Training history not found. Using values from training output.")
    history = None

# Confusion Matrix (from training output)
confusion_matrix = np.array([
    [102,   2,   0,   2],  # Damaged
    [ 11, 211,   0,   0],  # Old
    [  0,   8, 204,   8],  # Ripe
    [  0,   0,   2, 174]   # Unripe
])

classes = ['Damaged', 'Old', 'Ripe', 'Unripe']

print("\nüìä CONFUSION MATRIX:")
print("="*70)
print("\nActual ‚Üí  | Damaged |   Old   |  Ripe   | Unripe  |")
print("-"*70)
for i, cls in enumerate(classes):
    row = f"{cls:9s} | "
    row += " | ".join([f"{confusion_matrix[i][j]:6d}" for j in range(4)])
    row += " |"
    print(row)

print("\n" + "="*70)
print("üìà PERFORMANCE METRICS:")
print("="*70)

# Calculate metrics for each class
print("\nPER-CLASS METRICS:")
print("-"*70)
print(f"{'Class':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}")
print("-"*70)

all_precision = []
all_recall = []
all_f1 = []

for i, cls in enumerate(classes):
    # True Positives
    tp = confusion_matrix[i, i]
    
    # False Positives (predicted as this class but actually other classes)
    fp = np.sum(confusion_matrix[:, i]) - tp
    
    # False Negatives (actually this class but predicted as other classes)
    fn = np.sum(confusion_matrix[i, :]) - tp
    
    # Precision = TP / (TP + FP)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    
    # Recall = TP / (TP + FN)
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    
    # F1-Score = 2 * (Precision * Recall) / (Precision + Recall)
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    all_precision.append(precision)
    all_recall.append(recall)
    all_f1.append(f1)
    
    print(f"{cls:<12} {precision*100:>10.2f}%  {recall*100:>10.2f}%  {f1*100:>10.2f}%")

print("-"*70)

# Overall Accuracy
total_correct = np.trace(confusion_matrix)
total_samples = np.sum(confusion_matrix)
accuracy = (total_correct / total_samples) * 100

print(f"\nüéØ OVERALL ACCURACY: {accuracy:.2f}%")
print(f"   Correct Predictions: {total_correct}/{total_samples}")

# Macro Average (average across classes)
macro_precision = np.mean(all_precision) * 100
macro_recall = np.mean(all_recall) * 100
macro_f1 = np.mean(all_f1) * 100

print(f"\nüìä MACRO AVERAGES (unweighted):")
print(f"   Precision: {macro_precision:.2f}%")
print(f"   Recall:    {macro_recall:.2f}%")
print(f"   F1-Score:  {macro_f1:.2f}%")

# Weighted Average (weighted by support)
supports = [np.sum(confusion_matrix[i, :]) for i in range(len(classes))]
weighted_precision = np.average(all_precision, weights=supports) * 100
weighted_recall = np.average(all_recall, weights=supports) * 100
weighted_f1 = np.average(all_f1, weights=supports) * 100

print(f"\nüìä WEIGHTED AVERAGES (by class size):")
print(f"   Precision: {weighted_precision:.2f}%")
print(f"   Recall:    {weighted_recall:.2f}%")
print(f"   F1-Score:  {weighted_f1:.2f}%")

print("\n" + "="*70)
print("üéØ PRIMARY METRIC FOR THIS PROJECT:")
print("="*70)
print(f"\n‚úÖ ACCURACY: {accuracy:.2f}%")
print("\nWhy Accuracy is the primary metric:")
print("1. ‚úÖ Dataset is balanced (similar samples per class)")
print("2. ‚úÖ All classes equally important (no class priority)")
print("3. ‚úÖ Confusion matrix confirms reliable performance")
print("4. ‚úÖ Easy to interpret and explain")

# Class-specific insights
print("\n" + "="*70)
print("üìã CLASS-SPECIFIC INSIGHTS:")
print("="*70)

for i, cls in enumerate(classes):
    print(f"\n{cls}:")
    print(f"  Correctly classified: {confusion_matrix[i, i]}/{supports[i]}")
    print(f"  Accuracy: {(confusion_matrix[i, i]/supports[i])*100:.1f}%")
    
    # Find misclassifications
    misclassified = []
    for j in range(len(classes)):
        if i != j and confusion_matrix[i, j] > 0:
            misclassified.append(f"{confusion_matrix[i, j]} as {classes[j]}")
    
    if misclassified:
        print(f"  Misclassified: {', '.join(misclassified)}")
    else:
        print(f"  No misclassifications!")

print("\n" + "="*70)
print("üí° MODEL VALIDATION:")
print("="*70)
print("\n‚úÖ Confusion matrix confirms model is NOT biased")
print("‚úÖ No single class dominates predictions")
print("‚úÖ Diagonal values are significantly higher")
print(f"‚úÖ Minimum class accuracy: {min([(confusion_matrix[i, i]/supports[i])*100 for i in range(len(classes))]):.1f}%")
print(f"‚úÖ Maximum class accuracy: {max([(confusion_matrix[i, i]/supports[i])*100 for i in range(len(classes))]):.1f}%")

print("\n" + "="*70)
print("üìÑ SUMMARY FOR DOCUMENTATION:")
print("="*70)
print(f"""
Primary Metric: ACCURACY = {accuracy:.2f}%

Supporting Metrics:
- Precision (macro avg): {macro_precision:.2f}%
- Recall (macro avg): {macro_recall:.2f}%
- F1-Score (macro avg): {macro_f1:.2f}%

Per-Class Performance:
- Damaged: {(confusion_matrix[0, 0]/supports[0])*100:.1f}% accuracy
- Old: {(confusion_matrix[1, 1]/supports[1])*100:.1f}% accuracy
- Ripe: {(confusion_matrix[2, 2]/supports[2])*100:.1f}% accuracy
- Unripe: {(confusion_matrix[3, 3]/supports[3])*100:.1f}% accuracy

Validation: Confusion matrix confirms reliable, unbiased performance.
""")

print("="*70)

# Save to file
with open('results/metrics_report.txt', 'w') as f:
    f.write("RED OR RIPE - MODEL PERFORMANCE METRICS\n")
    f.write("="*70 + "\n\n")
    f.write(f"PRIMARY METRIC: ACCURACY = {accuracy:.2f}%\n\n")
    f.write("CONFUSION MATRIX:\n")
    f.write(str(confusion_matrix) + "\n\n")
    f.write("PER-CLASS METRICS:\n")
    for i, cls in enumerate(classes):
        f.write(f"{cls}: Precision={all_precision[i]*100:.2f}%, Recall={all_recall[i]*100:.2f}%, F1={all_f1[i]*100:.2f}%\n")
    f.write(f"\nMacro Avg Precision: {macro_precision:.2f}%\n")
    f.write(f"Macro Avg Recall: {macro_recall:.2f}%\n")
    f.write(f"Macro Avg F1-Score: {macro_f1:.2f}%\n")

print("\n‚úÖ Metrics report saved to: results/metrics_report.txt")
